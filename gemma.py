import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from PIL import Image

# üìå Choix du mod√®le (Gemma-like)
MODEL_NAME = "google/gemma-2b-it"  # ou tout mod√®le causallm compatible HF

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)
    return tokenizer, model

st.title("ü¶ô Chat avec Gemma")
st.write("Entrez une instruction ou ajoutez une image pour enrichir l'exp√©rience.")

tokenizer, model = load_model()

# Texte d‚Äôentr√©e
prompt = st.text_area("üí¨ Entrez une instruction :", height=100)

# Upload image (affichage seulement, pas encore trait√©e par le mod√®le)
image = st.file_uploader("üì∑ Optionnel : chargez une image", type=["jpg", "jpeg", "png"])
if image:
    st.image(Image.open(image), caption="Image charg√©e", use_column_width=True)

# Bouton de g√©n√©ration
if st.button("G√©n√©rer une r√©ponse") and prompt:
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    st.markdown("### ü§ñ R√©ponse :")
    st.write(response)
